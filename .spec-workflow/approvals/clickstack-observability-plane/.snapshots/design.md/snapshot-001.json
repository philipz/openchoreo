{
  "id": "snapshot_1763048513910_fb7j2lphc",
  "approvalId": "approval_1763048513900_7zxxz27ba",
  "approvalTitle": "ClickStack observability design",
  "version": 1,
  "timestamp": "2025-11-13T15:41:53.910Z",
  "trigger": "initial",
  "status": "pending",
  "content": "# Design Document\n\n## Overview\nWe will refactor the `install/helm/openchoreo-observability-plane` chart and supporting services so that it deploys ClickStack (ClickHouse + HyperDX UI + OTLP gateway) instead of OpenSearch. The migration introduces a reusable telemetry module inside the Go control plane that abstracts storage providers, updates the Observer API to issue ClickHouse SQL, and adds OTLP collector assets that enrich data with OpenChoreo metadata. The work follows the architecture blueprint described in `OpenSearch2ClickStack.md`.\n\n## Steering Document Alignment\n\n### Technical Standards (tech.md)\n- Implements the ClickStack-first technology stack (Go 1.24 services, Helm-packaged collectors, ClickHouse storage, HyperDX UI, Grafana overlays) defined in `.spec-workflow/steering/tech.md`.\n- Reuses controller-runtime patterns for Kubernetes-facing automation and keeps OTLP-first pipelines with mTLS and row-level security.\n\n### Project Structure (structure.md)\n- New code lives under `internal/telemetry/clickstack` and `pkg/telemetry` to respect plane isolation.\n- Helm/Kustomize assets land in `config/observability/` and the existing `install/helm/openchoreo-observability-plane` chart with overlays per environment.\n- Observer API adapters and Backstage integrations consume interfaces rather than embedding ClickStack specifics, matching the module boundary guidance.\n\n## Code Reuse Analysis\n- **internal/observer/service**: Keep handler layer and request/response DTOs; swap storage interface to ClickStack implementation.\n- **pkg/config + pkg/auth**: Reuse configuration loading and SPIFFE-based mTLS helpers for securing OTLP collectors and Observer API.\n- **make/kind.mk + install/init/observability**: Reuse bootstrap targets, extending them with ClickStack CRDs and collectors.\n\n### Existing Components to Leverage\n- **opensearch logging service** → provides interface contracts; we will factor them into `pkg/telemetry/query`.\n- **Fluent Bit / collector charts** → reuse templating patterns to deliver OTLP collector configs with minimal changes for metadata enrichment.\n\n### Integration Points\n- **Observer API** (`cmd/observer`, `internal/observer/handlers`) integrates with ClickStack via a new ClickHouse client built atop the official driver or `github.com/ClickHouse/clickhouse-go/v2`.\n- **Backstage plugins** consume Observer endpoints and new signed HyperDX URLs exposed through the Observer service.\n- **Grafana** dashboards ingest Prometheus metrics from ClickHouse, collectors, and the Observer API.\n\n## Architecture\nThe solution deploys ClickStack components alongside OTLP collectors and updates the Observer API to query ClickHouse.\n\n```mermaid\ngraph TD\n    subgraph DataPlane[\"Data Plane Clusters\"]\n        FB[Fluent Bit]\n        OC[OpenTelemetry Collector DaemonSet]\n    end\n    FB --> OC\n    OC -->|OTLP gRPC/HTTP| OGW[Observability Gateway Service]\n    OGW --> CH[ClickHouse Cluster]\n    CH --> HDX[HyperDX UI]\n    CH --> OBS[Observer API (Go)]\n    OBS -->|REST/GraphQL| BP[Backstage Plugins]\n    OBS --> CLI[choreoctl]\n    OBS --> GRA[Grafana Datasources]\n```\n\n- Collectors capture logs/traces/metrics, enrich them with org/project labels, and push to the ClickStack ingestion service running inside the observability plane.\n- ClickHouse stores raw telemetry plus materialized views (service map, RED metrics).\n- HyperDX UI serves traces/logs, while Grafana pulls down-metric views.\n- Observer API queries ClickHouse via SQL templates and signs dashboard URLs for Backstage.\n\n### Modular Design Principles\n- Each Go package focuses on a single concern (storage adapters, handlers, query builders).\n- Helm templates split by component (clickhouse-cluster, hyperdx-ui, collectors, grafana, ops jobs).\n- Service layer separation: handlers → service (`pkg/telemetry/service`) → storage adapters (`internal/telemetry/clickstack`).\n- Utility modules (query builders, schema registry) grouped under `pkg/telemetry/query`.\n\n## Components and Interfaces\n\n### Component 1 — ClickStack Storage Adapter (`internal/telemetry/clickstack`)\n- **Purpose:** Provide CRUD/query operations for logs, traces, metrics using ClickHouse SQL and wrap connection pooling, retries, and observability.\n- **Interfaces:** Implements `pkg/telemetry/query.StorageProvider` with methods like `FetchComponentLogs(ctx, filters)`, `FetchTraces(ctx, spanFilters)`.\n- **Dependencies:** ClickHouse Go driver, configuration from `pkg/config`, TLS credentials from `pkg/auth`.\n- **Reuses:** DTOs and filter structs currently used by the OpenSearch service.\n\n### Component 2 — Observer API Service Layer (`internal/observer/service`)\n- **Purpose:** Mediate between HTTP handlers and storage adapters, handle dual-read, caching, and response shaping.\n- **Interfaces:** `NewLoggingService(storage StorageProvider, opts ServiceOptions)`; exposes `GetComponentLogs`, `GetProjectLogs`, etc.\n- **Dependencies:** Storage provider interface, feature flag module, metrics emitter.\n- **Reuses:** Existing handlers (`internal/observer/handlers`) with minimal changes.\n\n### Component 3 — Helm Chart Modules (`install/helm/openchoreo-observability-plane`)\n- **Purpose:** Deploy ClickHouse cluster (statefulset + keeper), HyperDX UI, OTLP gateway, collectors, Grafana dashboards.\n- **Interfaces:** Helm values (`profiles.standard`, `profiles.minimal`, `clickstack.storage.*`, `collectors.otel.*`).\n- **Dependencies:** ClickHouse operator CRDs (optional), Kubernetes secrets/certs, cert-manager for TLS.\n- **Reuses:** Chart structure, RBAC templates, helper templates, readiness jobs (updated for ClickStack health checks).\n\n### Component 4 — Migration Jobs & Shadow Writer\n- **Purpose:** Manage shadow write/cutover and cleanup of OpenSearch indices.\n- **Interfaces:** Helm hooks or Kubernetes Jobs triggered via `helm upgrade --set migration.shadow=true`.\n- **Dependencies:** Fluent Bit configs, OTLP collector config maps, object storage for backups.\n- **Reuses:** Existing `opensearch-readiness-job` pattern, now pointing to ClickStack endpoints.\n\n## Data Models\n\n### Telemetry Filter DTO (Go)\n```\ntype LogQuery struct {\n    ComponentID   string\n    ProjectID     string\n    OrgID         string\n    TimeRange     query.TimeRange\n    Severity      []string\n    SearchText    string\n    Limit         int\n    Cursor        string // for pagination\n}\n```\n\n### ClickHouse Materialized View Schema\n```\nCREATE TABLE telemetry.logs_mv (\n    Timestamp DateTime64(9),\n    OrgID LowCardinality(String),\n    ProjectID LowCardinality(String),\n    ComponentID LowCardinality(String),\n    Severity LowCardinality(String),\n    Message String,\n    Attributes Map(LowCardinality(String), String),\n    K8sNamespace String,\n    PodName String,\n    TraceID String,\n    SpanID String\n) ENGINE = ReplacingMergeTree()\nPARTITION BY toDate(Timestamp)\nORDER BY (OrgID, ProjectID, ComponentID, Timestamp)\nTTL Timestamp + INTERVAL 90 DAY;\n```\n\n## Error Handling\n\n### Error Scenario 1 — ClickHouse connectivity failure\n- **Handling:** Storage adapter retries with exponential backoff, surfaces gRPC status codes to handlers, emits Prometheus metrics `telemetry_clickstack_connection_errors_total`.\n- **User Impact:** Observer API returns HTTP 503 with actionable error message (`storage backend unavailable`) while dashboards show degraded banner.\n\n### Error Scenario 2 — Query timeout / heavy scan\n- **Handling:** Query builder enforces max timeout (e.g., 5s) and row limit; if exceeded, it cancels context and logs the offending filters for analysis.\n- **User Impact:** API responds with 504 plus hint to tighten time range; HyperDX UI surfaces partial results.\n\n### Error Scenario 3 — Metrics breach triggers autoscaling\n- **Handling:** Helm chart installs HorizontalPodAutoscaler / ClickHouse Keeper scaling jobs; if scaling fails, alert rules notify operators.\n- **User Impact:** None if auto-scaling succeeds; otherwise, status dashboard flags capacity risk.\n\n## Testing Strategy\n\n### Unit Testing\n- Add tests for `pkg/telemetry/query` to validate SQL generation (filters, pagination, RLS hints).\n- Mock ClickHouse client to ensure dual-read logic and translation functions return OpenSearch-compatible payloads.\n- Validate Helm chart helper templates via `helm template` + `kubeconform` in CI.\n\n### Integration Testing\n- Extend `test/e2e` KinD suite to deploy ClickStack profile, seed synthetic telemetry via OTLP, and validate Observer API endpoints return expected data.\n- Run migration shadow tests: simultaneously ingest into OpenSearch & ClickStack, diff results (tolerate <1% delta).\n\n### End-to-End Testing\n- Scenario: Developer runs `choreoctl logs component <id>` in a KinD environment; test asserts CLI output matches inserted log lines and that Grafana dashboards render.\n- Scenario: FinOps dashboard collects cost metrics; verify exported CSV structure and totals.\n",
  "fileStats": {
    "size": 8873,
    "lines": 151,
    "lastModified": "2025-11-13T15:41:49.709Z"
  },
  "comments": []
}